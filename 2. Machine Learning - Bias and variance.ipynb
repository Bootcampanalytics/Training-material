{
    "metadata": {
        "kernelspec": {
            "name": "python3", 
            "display_name": "Python 3.5 (Experimental) with Spark 1.6", 
            "language": "python"
        }, 
        "language_info": {
            "version": "3.5.2", 
            "file_extension": ".py", 
            "nbconvert_exporter": "python", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }, 
            "name": "python", 
            "mimetype": "text/x-python", 
            "pygments_lexer": "ipython3"
        }
    }, 
    "cells": [
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "# Dealing with Bias and Variance\n\nAdopted from https://github.com/addfor/tutorials"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": 2, 
            "source": "import scipy.io\nimport numpy as np\nimport pandas as pd\nfrom time import time\nfrom sklearn import metrics\nimport sys\nimport os\nfrom IPython.core.display import Image\n"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "import bokeh.plotting as bk\nbk.output_notebook()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## 1 Bias-Variance Tradeoff"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "### 1.1 (In)Formal introduction to Bias and Variance\n\nRoughly speaking Machine Learning algorithms try to characterize a tradeoff between generalization (how well the model represents the general concept or function to learn) and apporoximation (how well the model fits the data). A small error denotes a good approximation of the function _out of sample_. A more complex model has a better chance of approximating $f$, in contrast a less complex model has a better chance of generalizing (picking the right approximation) out of sample.\n\nA more formal definition can be given after *Hastie, et al. 2009*.\nLet's explain Bias-Variance using linear regression problems, and specifically we'll use a squared-error measure as loss function. Assume further that the output $Y = f(X) + \\epsilon$ where $E(\\epsilon) = 0$ and $Var(\\epsilon) = \\sigma_\\epsilon^2$\n\nWe estimate a model $\\widehat{f}(X)$ of a function $f(X)$, and the expected squared error of a given point $x_0$ is:\n\n$$Err(x_0) = E[(Y - \\widehat{f}(x_0))^2]$$\n\nIt is possible to decompose this error in its Bias and Variance components:\n\n$$Err(x_0) = \\big(E[\\widehat{f}(x_0)] - f(x_0)\\big)^2 + E\\big[\\widehat{f}(x_0) - E[\\widehat{f}(x_0)]\\big]^2 + \\sigma_{\\epsilon}^2$$\n\n$$Err(x_0) = Bias^2 + Variance + Irreducible Error$$\n\nWhere $Irreducible Error$ is the variance in the true relationship that cannot be captured by any model no matter how well we estimate $f(X)$. $Bias^2$ is the difference between the average of our estimate from the true mean. $Variance$ is the expected squared deviation of $\\widehat{f}(x_0)$ around its mean. Usually the more complex is the model the lower the Bias but the higher the variance. We can give a pictorial illustration of the relationship with the image below. (The image is taken from [this](http://scott.fortmann-roe.com/docs/BiasVariance.html) blog)"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "Image(url= \"https://github.com/analytics-bootcamp/day-1/blob/master/resources/bias%20and%20variance.png?raw=true\")"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Suppose the center (red) of each diagram is the real function to learn. As we move away from the center the prediction gets worse and worse. Each dot in the picture is a realization of the model given a subsample of the dataset; each circle is a different model (whose particular realizations are the dots) and model's characteristics are described by the two corresponding labels (horizontal and vertical).\n\nHaving a model (top left) with both Low Bias and Low Variance is practically impossible since, as we will see next, if we decrese one the other increase (in this case both generalization and approximation will be good and we could learn almost the real function); the hard part is hence finding the best possible balance between the two. In case of Low Bias and High Variance (top right), the model is capable of representing the correct function but the model may be too complex, and thus more supsceptible in variation on the dataset. For this reason each dot (function approximation) draw from this model may vary considerably from estimate to estimate, depending on the particular subsample used for training. For High Bias and Low Variance (bottom left) the model is constantly learning a substantially different function respect to the true one, but each run is independent from the particular subsample of data (it generalize well but it approximate poorly). If we have a model that has both High Varianche and High Bias (bottom right) the model si constantly learning a wrong model with high variability between subsamples and thus the probability of picking a good model is very low. \n\nOne can think of Bias and Variance as urn that contains a bunch of candidate approximations of an unknown function $f$. If we have few candidates in the urn (think about degenerating and have only one function in it), but each candidate is far away from the true value of $f$; in this case the Bias is high and the Variance is low, no matter of the dataset we will always return the same function. If we have a bigger urn containing a lot of candidates the probability that it contains the true function is higher, but it is difficult to navigate through the candidates to find the right one. In this case the Bias is lower but the Variance is higher, we have so many varieties that depending on which sample we get the function differs.\n\nNext we will show what we mean by complexity of the model with some practical examples in order to further understand the Bias-Variance tradeoff."
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": 4, 
            "source": "url=\"https://raw.githubusercontent.com/analytics-bootcamp/day-1/master/resources/biasvariance.py\"\nfilename=\"biasvariance.py\"\n\nimport urllib.request\nurllib.request.urlretrieve(url, filename)\n\nimport biasvariance\nfrom bokeh.models.ranges import Range1d"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "In this figure, we use polynomials with different degrees `d` to fit the same data.\n\nFor `d=1`, the model suffers from **high bias**: the model complexity is not enough to represent the data.\n\nAt the other extreme, for `d=6` the model suffers from **high variance**: the mode overfit the data because it has too many free parameters.  In this case a typical behaviour is that if any of the input points are varied slightly, it could result in a very different approximation. This kind of model will fit perfectly the training data while failing to do so on validation data.\n\nIn the middle picture, for `d=2`, we have found a good model."
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": 5, 
            "source": "%matplotlib inline\nimport matplotlib.pyplot as plt"
        }, 
        {
            "metadata": {
                "scrolled": true, 
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [
                {
                    "text": "/gpfs/fs01/user/s16e-7918d85e6de098-7a7840b6cba3/.local/lib/python3.5/site-packages/bokeh/util/deprecation.py:34: BokehDeprecationWarning: Plot.title_text_font_size was deprecated in Bokeh 0.12.0 and will be removed, use Plot.title.text_font_size instead.\n  warn(message)\n/gpfs/fs01/user/s16e-7918d85e6de098-7a7840b6cba3/.local/lib/python3.5/site-packages/bokeh/util/deprecation.py:34: BokehDeprecationWarning: Plot.title_text_font_size was deprecated in Bokeh 0.12.0 and will be removed, use Plot.title.text_font_size instead.\n  warn(message)\n/gpfs/fs01/user/s16e-7918d85e6de098-7a7840b6cba3/.local/lib/python3.5/site-packages/bokeh/util/deprecation.py:34: BokehDeprecationWarning: Plot.title_text_font_size was deprecated in Bokeh 0.12.0 and will be removed, use Plot.title.text_font_size instead.\n  warn(message)\n", 
                    "name": "stderr", 
                    "output_type": "stream"
                }, 
                {
                    "ename": "PermissionError", 
                    "traceback": [
                        "\u001b[1;31m\u001b[0m", 
                        "\u001b[1;31mPermissionError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[1;32m<ipython-input-6-80fec4b271ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbiasvariance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_bias_variance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[1;32m/gpfs/global_fs01/sym_shared/YPProdSpark/user/s16e-7918d85e6de098-7a7840b6cba3/notebook/work/biasvariance.py\u001b[0m in \u001b[0;36mplot_bias_variance\u001b[1;34m(N, random_seed, err)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mgp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgridplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mborder_space\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mbk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/gpfs/fs01/user/s16e-7918d85e6de098-7a7840b6cba3/.local/lib/python3.5/site-packages/bokeh/io.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(obj, browser, new, notebook_handle)\u001b[0m\n\u001b[0;32m    298\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroots\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[0m_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_root\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_show_with_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnotebook_handle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnotebook_handle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/gpfs/fs01/user/s16e-7918d85e6de098-7a7840b6cba3/.local/lib/python3.5/site-packages/bokeh/io.py\u001b[0m in \u001b[0;36m_show_with_state\u001b[1;34m(obj, state, browser, new, notebook_handle)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshown\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m         \u001b[0m_show_file_with_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontroller\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcomms_handle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/gpfs/fs01/user/s16e-7918d85e6de098-7a7840b6cba3/.local/lib/python3.5/site-packages/bokeh/io.py\u001b[0m in \u001b[0;36m_show_file_with_state\u001b[1;34m(obj, state, new, controller)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_show_file_with_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontroller\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m     \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m     \u001b[0mcontroller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"file://\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_new_param\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/gpfs/fs01/user/s16e-7918d85e6de098-7a7840b6cba3/.local/lib/python3.5/site-packages/bokeh/io.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, filename, resources, title, state, validate)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresources\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_save_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresources\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m     \u001b[0m_save_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresources\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/gpfs/fs01/user/s16e-7918d85e6de098-7a7840b6cba3/.local/lib/python3.5/site-packages/bokeh/io.py\u001b[0m in \u001b[0;36m_save_helper\u001b[1;34m(obj, filename, resources, title, validate)\u001b[0m\n\u001b[0;32m    447\u001b[0m         \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstandalone_html_page_for_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresources\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecode_utf8\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/usr/local/src/conda3_runtime/4.1.1/lib/python3.5/runpy.html'"
                    ], 
                    "evalue": "[Errno 13] Permission denied: '/usr/local/src/conda3_runtime/4.1.1/lib/python3.5/runpy.html'", 
                    "output_type": "error"
                }
            ], 
            "execution_count": 6, 
            "source": "biasvariance.plot_bias_variance(8, random_seed=42, err=10)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "### 1.2 Bias-Variance Tradeoff explained with a regression example"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "In this section we will explore a simple **linear regression** problem.\nThis can be accomplished within scikit-learn with the `sklearn.linear_model` module.\n\nWe consider the situation where we have only 2 data points: "
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "from sklearn import linear_model\nX = np.array([0.5, 1.0]).reshape(2,1)\ny = np.array([0.5, 1.0])\nX_test = np.array([0.0, 2.0]).reshape(2,1)\nregr = linear_model.LinearRegression()\nregr.fit(X, y)\nfig = bk.figure(plot_width=400, plot_height=300, title=None)\nfig.circle(X[:,0], y, size=5)\nfig.line(X_test[:,0], regr.predict(X_test), color='green')\nbk.show(fig)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "In real life situation, we have noise (e.g. measurement noise) in our data:"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "import seaborn as sns\nimport addutils.palette as pal\nimport random\ncat_colors = list(map(pal.to_hex, sns.color_palette('bright', 6)))\nrandom.shuffle(cat_colors)"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "np.random.seed(0)\nfig = bk.figure(plot_width=400, plot_height=300, title=None)\n\nfor i in range(6):\n    noise = np.random.normal(loc=0, scale=.1, size=X.shape)\n    noisy_X = X + noise\n    regr.fit(noisy_X, y)\n    \n    fig.circle(noisy_X[:,0], y, size=8, fill_color=cat_colors[i], line_color='black')\n    fig.line(X_test[:,0], regr.predict(X_test), color=cat_colors[i])\nbk.show(fig)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "As we can see, our linear model captures and amplifies the noise in the data. It displays a lot of variance. This means that if we subsample the data and we fit a different model to each subsample (each pair of points) we obtain a model that differ substantially from the true model. There are several techniques to reduce variance. **Regularization** is one of those, it is a way to change the tradeoff between Bias and Variance by putting more weights on either one. We will briefly use it in this example while we leave a more detailed explanation for the next section.\n\nWe can use another linear estimator that uses **regularization**: the Ridge estimator. This estimator regularizes the coefficients by shrinking them to zero, under the assumption that very high correlations are often spurious. High alphas give high regularization (shrinkage):"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "regr = linear_model.Ridge(alpha=0.08)\n\nfig = bk.figure(plot_width=400, plot_height=300, title=None)\nfor i in range(6):\n    noise = np.random.normal(loc=0, scale=.1, size=X.shape)\n    noisy_X = X + noise\n    regr.fit(noisy_X, y)\n    fig.circle(noisy_X[:,0], y, size=8, color=cat_colors[i], line_color='black')\n    fig.line(X_test[:,0], regr.predict(X_test), color=cat_colors[i])\nbk.show(fig)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "As we can see, the estimator displays much less variance. However it systematically under-estimates the coefficient. It displays a biased behavior. As explained earlier we changed the tradeoff between Bias and Variance with regularization. Changing one quantity (for example decreasing Variance) inevitably increases the other (Bias). \n\nWith the next examples we will try to answer the following question: **If our estimator is underperforming, how should we move forward?**\n\n- Do I need a Simple or more Complicated Model ?\n- Do I need More Training Samples ?\n- Do I need more features for each observed data point ?\n\nThe answer is often counter-intuitive. In particular, **Sometimes using a more complicated model will give _worse_ results.**  Also, **Sometimes adding training data will not improve your results.**  The ability to determine what steps will improve your model is what separates the successful machine learning practitioners from the unsuccessful."
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## 2 Regularization: what it is and why it is necessary"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "**The core idea behind regularization is that we are going to prefer models that are simpler**, even if they lead to more errors on the training set. We start by defining a 9th order polynomial function. This represents our 'ground truth'. You can imagine it like a signal measured at diffrent times:"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "f = lambda t: 1.2*t**2 + 0.1*t**3 - 0.6*t**5 - 0.8*t**9\ngt_coeff = [0, 0, 1.2, 0.1, 0., -0.6, 0., 0., 0., -0.8]"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Unfortunately in real life situation every signal is affected by a measurement error; in this example we simulate it with the variable `noise_level`. Our ground truth is a 9th order polynomial.\n\nAt first it would seem an obvius choice to use a 9th order polynomial to fit the signal. If you play a little with the following code you will discover that not using a regularization technique (`LinearRegression` doesn't allow any regularization), most of the time could be a bad choice, and a simpler (lower-order) model it is much better to avoid overfitting.\n\nTry to change the following variables:\n\n* `orders`: orders of the polynomials to fit\n* `n_samples`: when the number of the samples is small it's difficult to fit a high-order model and you have overfitting\n* `noise_level`: whit very low noise it's easier to fit higher-order polynomials\n\n*Keep in mind that we didn't fix the random generator seed, so every time you run the cell you'll have a different noise distribution on the samples*.\n\n**As you can see we use a linear algorithm to fit a nonlinear function, this is possible because we use the linear algorithm to fit the nonlinear coefficients that we define in the regressors**."
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "tmin, tmax = -1.1, 1.0\nn_samples = 25\nnoise_level = 0.2\norders = [4, 9]\nnp.random.seed(1)\nt = tmin + np.random.rand(n_samples) * (tmax-tmin)              # Sample points\ny_noisy = f(t) + noise_level*np.random.normal(size=n_samples)   # Noisy measure\n\nfig = bk.figure(plot_width=700, plot_height=400,\n                title='4th and a 9th order polynomial with Ground truth')\nfig.circle(t, y_noisy, size=8, fill_alpha=0.4)\n\np = np.linspace(tmin, tmax, 200)                                # Array to calc. the prediction\ncolors = ['royalblue', 'green']\nfor order in orders:\n    X = np.array([t**i for i in range(order+1)]).T              # Regressor\n    Xp = np.array([p**i for i in range(order+1)]).T             # Regressor for prediction\n    poly_linreg = linear_model.LinearRegression().fit(X, y_noisy)\n    fig.line(p, poly_linreg.predict(Xp), legend='linreg order: %02i' % order,\n             color=colors.pop(), line_width=3.0)\n\nfig.line(p, f(p), legend=\"truth\", color='red', line_width=4.0)\nfig.legend.label_text_font_size = '14pt'\nfig.legend.location = 'top_left'\nnp.set_printoptions(precision=2)\nprint('Ground Truth coeff.: ', ' '.join(['%+5.1f' %n for n in gt_coeff]))\nprint('LinReg coefficients: ', ' '.join(['%+5.1f' %n for n in poly_linreg.coef_]))\nbk.show(fig)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Now we compare the previous `LinearRegression` algorithm with the `Lasso` algorithm.\n\n`Lasso` (least absolute shrinkage and selection operator) is an alternative regularized version of least squares: it is a **shrinkage estimator**: unlike `ridge regression`, as the penalty term increases, lasso sets more coefficients to zero, this means that the lasso estimator produces smaller models."
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "order = 9\nnp.random.seed(1)\nX = np.array([t**i for i in range(order+1)]).T\nXp = np.array([p**i for i in range(order+1)]).T\npoly_linreg = linear_model.LinearRegression().fit(X, y_noisy)\npoly_lasso = linear_model.Lasso(alpha = 0.005).fit(X, y_noisy)\n\nfig = bk.figure(plot_width=700, plot_height=400)\nfig.circle(t, y_noisy, size=8, alpha=0.4)\nfig.line(p, poly_linreg.predict(Xp), legend='linreg order: %02i' % order,\n         color='royalblue', line_width=3.0)\nfig.line(p, poly_lasso.predict(Xp), legend='lasso order: %02i' % order,\n         line_width=3.0, color='green')\nfig.line(p, f(p), legend=\"truth\", color='red', line_width=4.0)\n\nfig.legend.label_text_font_size = '15pt'\nfig.legend.location = 'top_left'\nfig.title = '9th order LinReg and Lasso with Ground truth'\n\nnp.set_printoptions(precision=2)\nprint('Ground Truth coeff.: ', ' '.join(['%+5.1f' %n for n in gt_coeff]))\nprint('LinReg coefficients: ', ' '.join(['%+5.1f' %n for n in poly_lasso.coef_]))\nbk.show(fig)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Try by yourself: play a little with the code of the two previous cells by changing `n_samples` and `noise_level` to verify that:\n\n* When `n_samples` is very high (>1000) it's very unlikely to have overfitting. This means that with more data it's possible to fit more complex models without overfitting.\n* When `n_samples` is very low (<40) it's almost impossible to fit with Linear Regression without overfitting. In this case regularization is always required\n* When `n_samples` is very high (>1000) and `noise_level` is very low, the Lasso algorithm keeps just the coefficients actually used to calculate the ground truth function. This means that Lasso can deal with high dimesional problems where most of the features can be neglected producing compact linear models.\n\nAnother way to look at the Bias-Variance tradeoff in previous examples is to consider how the model fits the data. As stated at the beginning of this section when model complexity increases Bias decrease while Variance increses. The optimal model performance is achieved when the level of complexity is such that an increase in Bias is equivalent in reduction of Variance. If the model complexity exceed this threshold we are overfitting (the model is to adapted to the data and does not generalize well) if the model is behind this threshold is said to be underfitting (the model does not fit well the data and it has a poor error for out of sample data)."
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## 3 Do I need a Simple or a Complex Model?"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "In order to show the relation between the size of the training dataset and the complexity of the model we generate some example data in the same way we did in the very first example but with more than ten times the number of samples. The we fit a polynomial model and we plot the error w.r.t. the degree of polynomial.\n\nHere **Training Data** are plotted in **Blue** while **Validation Data** are **Red**:"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "from sklearn import cross_validation\nN = 100\ntest_size = 0.40\nerror = 1.0\nnp.random.seed(1)\nx = np.random.random(N)\ny = biasvariance.test_func(x, error)\nx_tr, x_valid, y_tr, y_valid = cross_validation.train_test_split(x, y, test_size=test_size)\nfig = bk.figure(plot_width=700, plot_height=400, title=None)\nfig.circle(x_tr, y_tr, color='blue', size=6, alpha=0.5)\nfig.circle(x_valid, y_valid, color='red', size=6, alpha=0.5)\nbk.show(fig)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "The **model parameters** *(in our case, the coefficients of the polynomials)* are learned using the **training set**.\n\nThe **meta-parameters** *(in our case, the degree of the polynomial)* are adjusted so that this **validation error** is minimized.\n\nFinally, the labels are predicted for the **test set**.\n\nThe **validation error** of our polynomial classifier can be visualized by plotting the error as a function of the polynomial degree:"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "import warnings\nwarnings.filterwarnings('ignore', message='Polyfit*')\n\ndegrees = np.arange(41)\ntrain_err = np.zeros(len(degrees))\nvalidation_err = np.zeros(len(degrees))\n\nfor i, d in enumerate(degrees):\n    p = np.polyfit(x_tr, y_tr, d)\n    train_err[i] = biasvariance.compute_error(x_tr, y_tr, p)\n    validation_err[i] = biasvariance.compute_error(x_valid, y_valid, p)\n\nfig = bk.figure(plot_width=700, plot_height=400, title=None)\n\nfig.line(degrees, validation_err,\n         line_width=2, legend='cross-validation error', color='royalblue')\nfig.line(degrees, train_err, line_width=2, legend='training error', color='green')\nfig.line([0, 20], [error, error], line_dash='dashed', legend='intrinsic error', color='black')\n\nfig.xaxis.axis_label = 'degree of fit'\nfig.yaxis.axis_label = 'rms error'\n\nfig.grid.grid_line_color = None\n\nbk.show(fig)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "For this toy dataset, error = 1.0 is the best we can hope to obtain. Choosing `d=6` in this case gets us very close to the optimal error.\n\nNotice that in the above plot, `d=6` gives the best results. But in the very first example, we found that `d=6` vastly over-fits the data. What\u2019s going on here? The difference is the **number of training points** used:\n\n**As a general rule of thumb, the more the training points, the more complex the model can be.**"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## 4 Do I need More Training Samples?"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "In order to exploit the relationship outlined above, between the complexity of the model and the number of examples, we use a tool called **Learning Curves**. Learning Curves works by plotting the training error and validation (or test) error as a **function of the number of training points**:"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "def plot_learning_curve(d, N=100, y_range=None):\n    test_size = 0.40\n    n_sizes = 50       # Number of testing point in which to split the size\n    n_runs = 20        # Number of times to run the test for each training set size\n    sizes = np.linspace(2, N, n_sizes).astype(int)\n    train_err = np.zeros((n_runs, n_sizes))\n    validation_err = np.zeros((n_runs, n_sizes))\n    for run in range(n_runs):\n        for nsize, size in enumerate(sizes):\n            x_tr, x_valid, y_tr, y_valid = cross_validation.train_test_split(x, y,\n                                           test_size=test_size, random_state=run)\n            # Train on only the first `size` points\n            p = np.polyfit(x_tr[:size], y_tr[:size], d)\n            # Validation error is on the *entire* validation set\n            validation_err[run, nsize] = biasvariance.compute_error(x_valid, y_valid, p)\n            # Training error is on only the points used for training\n            train_err[run, nsize] = biasvariance.compute_error(x_tr[:size], y_tr[:size], p)\n\n    fig = bk.figure(plot_width=400, plot_height=300, title='d = %i' % d,\n                    x_range=(0, N-1)) \n    fig.title_text_font_size = '11pt'\n    fig.xaxis.axis_label_text_font_size = '9pt'\n    fig.yaxis.axis_label_text_font_size = '9pt'\n    \n    fig.line(sizes, validation_err.mean(axis=0), \n             line_width=2, legend='mean validation error', color='royalblue')\n    fig.line(sizes, train_err.mean(axis=0),\n             line_width=2, legend='mean training error', color='green')\n    fig.line([0, N], [error, error],\n             line_dash='dashed', legend='intrinsic error', color='black')\n    \n    fig.xaxis.axis_label = 'traning set size'\n    fig.yaxis.axis_label = 'rms error'\n    fig.legend.location = 'top_right'\n    \n    if y_range:\n        fig.y_range = Range1d(y_range[0], y_range[1])\n\n    fig.grid.grid_line_color = None\n    bk.show(fig)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Now we look at the behavior of <font color=\"royalblue\">$E_{train}$</font> and <font color=\"red\">$E_{test}$</font>.\n\n<font color=\"red\">As we increase $N$, $E_{test}$ decreases</font> until the limit of the algorithm, that is its $Bias$.\n\n<font color=\"royalblue\">As we increase $N$, $E_{train}$ increases</font>, because with few datapoints the task is simpler, while adding more data increase the difficulty of the algorithm to fit them.\n\nThe discrepancy of the two curves is the **generalization error**. It should shrink and get tighter as generalization increases."
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "execution_count": 3, 
                    "data": {
                        "text/html": "<img src=\"https://github.com/analytics-bootcamp/day-1/blob/master/resources/learning_curves.png?raw=true\"/>", 
                        "text/plain": "<IPython.core.display.Image object>"
                    }
                }
            ], 
            "execution_count": 3, 
            "source": "Image(url= \"https://github.com/analytics-bootcamp/day-1/blob/master/resources/learning_curves.png?raw=true\")"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "In the left picture (simple model) we can see that the model \"true\" error (the best the model can do) is high and pretty soon (that is for lower values of $N$) we reach the point were both $E_{train}$ and $E_{test}$ are closer. In the right picture (complex model) we can see that, since it is more complex, it has a better approximation of the target function and thus can achieve (in principle) a lower $E_{test}$ error. The function is so complex that with few data points it fits them perfectly reaching a training error of zero. However if we look at the corresponding $E_{test}$ we can see that it is very high, we haven't learn anything. As the number of examples grows, $E_{test}$ starts lowering and it gets lower than the other model, but the discrepancy between the two errors remains higher than that of the previous example. \n\nLet's see a practical example:"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "plot_learning_curve(d=1, N=100)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "A polynomial model with `d=1` is a **high-biased estimator** which under-fits the data. This is indicated by the fact that **both the training and validation errors are very high**.\n\n*Adding more data WON'T work in this case.*\n\n**When both TRAINING and VALIDATION curves converge to an high error, we have a HIGH BIAS model.** In this situation we can try one of the following actions:\n\n* Using a more sophisticated model (i.e. in this case, increase ``d``)\n* Gather more features for each sample.\n* Decrease regularlization.\n\nNow let's look at a model with higher degree (and thus higher variance):"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "plot_learning_curve(d=10, N=100, y_range=(0,12))"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "In this example it is possible to see that with `d=10` a **high-variance** estimator may **overfit data** when the number of samples is not sufficent. It is possible to note it by looking at the discrepancy between the two curves. When the number of samples is low we have a low training error, but the corresponding test error is really high. As the number of samples increase the test error decrease and converge to the model error but it may be to slow to converge and still have an higher discrepancy between the two curves.\n\n**When the learning curves does not converge using the full training set, it indicates a HIGH VARIANCE model, and the model is OVERFITTING the data.**\n\nA high-variance model can be improved by:\n\n* Gathering more training samples.\n* Using a less-sophisticated model (i.e. in this case, make `d` smaller)\n* Increase regularization.\n\n*In particular, gathering more features for each sample will not help the results.*"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## 6 Do the right thing"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "### 6.1 High Bias"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "If our algorithm shows high **bias**, the following actions might help:\n\n- **Add more features**. In our example of predicting home prices,\n  it may be helpful to make use of information such as the neighborhood\n  the house is in, the year the house was built, the size of the lot, etc.\n  Adding these features to the training and test sets can improve\n  a high-bias estimator\n- **Use a more sophisticated model**. Adding complexity to the model can\n  help improve on bias. For a polynomial fit, this can be accomplished\n  by increasing the degree d. Each learning technique has its own\n  methods of adding complexity.\n- **Use fewer samples**. Though this will not improve the classification,\n  a high-bias algorithm can attain nearly the same error with a smaller\n  training sample. For algorithms which are computationally expensive,\n  reducing the training sample size can lead to very large improvements\n  in speed.\n- **Decrease regularization**. Regularization is a technique used to impose\n  simplicity in some machine learning models, by adding a penalty term that\n  depends on the characteristics of the parameters. If a model has high bias,\n  decreasing the effect of regularization can lead to better results."
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "### 6.2 High Variance"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "If our algorithm shows **high variance**, the following actions might help:\n\n- **Use fewer features**. Using a feature selection technique may be\n  useful, and decrease the over-fitting of the estimator.\n- **Use a simpler model**.  Model complexity and over-fitting go hand-in-hand.\n- **Use more training samples**. Adding training samples can reduce\n  the effect of over-fitting, and lead to improvements in a high\n  variance estimator.\n- **Increase Regularization**. Regularization is designed to prevent\n  over-fitting. In a high-variance model, increasing regularization\n  can lead to better results.\n\nThese choices become very important in real-world situations. For example,\ndue to limited telescope time, astronomers must seek a balance between\nobserving a large number of objects, and observing a large number of\nfeatures for each object. Determining which is more important for a\nparticular learning task can inform the observing strategy that the\nastronomer employs. In a later exercise, we will explore the use of\nlearning curves for the photometric redshift problem."
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "### 6.3 More Sophisticate Methods"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "There are a lot more options for performing validation and model testing.\nIn particular, there are several schemes for cross-validation, in which\nthe model is fit multiple times with different training and test sets.\nThe details are different, but the principles are the same as what we've\nseen here.\n\nFor more information see the ``sklearn.cross_validation`` module documentation,\nand the information on the scikit-learn website."
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "### 6.4 One Last Caution"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Using validation schemes to determine hyper-parameters means that we are\nfitting the hyper-parameters to the particular validation set.  In the same\nway that parameters can be over-fit to the training set, hyperparameters can\nbe over-fit to the validation set.  Because of this, the validation error\ntends to under-predict the classification error of new data.\n\nFor this reason, it is recommended to split the data into three sets:\n\n- The **training set**, used to train the model (usually ~60% of the data)\n- The **validation set**, used to validate the model (usually ~20% of the data)\n- The **test set**, used to evaluate the expected error of the validated model (usually ~20% of the data)\n\n*This may seem excessive, and many machine learning practitioners ignore the need\nfor a test set.  But if your goal is to predict the error of a model on unknown\ndata, using a test set is vital*."
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## 7 Additional examples on Regularization: Ridge Regression and Stocastic Gradient Descent (SGD)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "`LinearRegression` fits a linear model with coefficients $w = (w_1, \\ldots, w_p)$ to minimize the residual sum of squares between the observed responses in the dataset, and the responses predicted by the linear approximation.\n\nMathematically it solves a problem of the form: \n<!-- <img src='files/utilities/ols_problem.png' > -->\n$$ \\underset{w}{min} \\|Xw -y\\|_2^2$$\n\nLet's see the `diabetes` dataset:"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "from sklearn import datasets\ndiabetes = datasets.load_diabetes() # Diabetes is a dataset with 442 samples and 10 attributes\nX = pd.DataFrame(diabetes.data)\ny = pd.DataFrame(diabetes.target)\n\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nX[X.columns[0:7]].head()            # Show the first 3 attributes for the first 7 samples"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "X[X.columns[0:7]].describe()       # Describe the first three columns"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "list(zip(X.columns, [type(x) for x in X.ix[0,:]]))  # Check the datatypes of the columns)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Keep just the attributes in the column number 2 (3rd column) and use 25% of the data as a testing set:"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "X_train, X_test, y_train, y_test = cross_validation.train_test_split(np.array(X[X.columns[2:3]]),\\\n                                                                     np.array(y),\\\n                                                                     test_size=0.25, random_state=0)\n"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "regr1 = linear_model.LinearRegression().fit(X_train, y_train)\n\nfig = bk.figure(plot_width=640, plot_height=300, title=None)\nfig.circle(X_test[:,0], y_test[:,0], color='black')\nfig.line(X_test[:,0], regr1.predict(X_test)[:,0], color='blue',\n         legend='linear regression', line_width=3)\nfig.legend.orientation = 'bottom_right'\nbk.show(fig)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "### 7.1 Ridge Regression"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Linear Regression rely on the independence of the model terms. When terms are correlated and the columns of the design matrix $X$; have an approximate linear dependence, the matrix $(X^TX)^{-1}$ <!--(&Chi;<sup>T</sup>&Chi;)<sup>-1</sup>--> becomes\nclose to singular. As a result, the least-squares estimate becomes highly sensitive to random errors in the observed response *y*, producing a large variance. This situation of multicollinearity can arise, for example, when data are collected without an experimental design.\n\nThe ridge coefficients minimize a penalized residual sum of squares:\n\n$$ \\underset{w}{min} \\|Xw -y\\|_2^2 + \\alpha \\|w\\|_2^2$$\n<!-- <img src='files/utilities/penalized_residuals_sum_of_squares.png'> -->\n\nHere, positive $\\alpha  \\geq 0 \\hspace{2 pt}$ is a complexity parameter that controls the amount of shrinkage: the larger the value of $\\alpha$, the greater the amount of shrinkage and thus the coefficients become more robust to collinearity."
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "regr2 = linear_model.Ridge(alpha = 0.5)\nregr2.fit(X_train, y_train)\n\nfig = bk.figure(plot_width=640, plot_height=330, title=None)\nfig.circle(X_test[:,0], y_test[:,0], color='black')\nfig.line(X_test[:,0], regr1.predict(X_test)[:,0], color='blue',\n         legend='linear regression', line_width=3)\nfig.line(X_test[:,0], regr2.predict(X_test)[:,0], color='red',\n         legend='ridge regression', line_width=3)\nfig.legend.orientation = 'bottom_right'\nbk.show(fig)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Keep all the 10 attributes and split training and testing sets:"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "X_train, X_test, y_train, y_test = cross_validation.train_test_split(np.array(X),\\\n                                                                     np.array(y),\\\n                                                                     test_size=0.25, \n                                                                     random_state=0)\n"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Fit the ridge regressor on all the attributes and calculate the fits for a given $\\alpha$"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "regr3 = linear_model.Ridge(alpha = 0.6)\nregr3.fit(X_train, y_train)\nprint('Coefficients: ', regr3.coef_[0][0])\nprint('Variance score: %.2f' % regr3.score(X_test, y_test))"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "The larger the value of $\\alpha  \\geq 0 \\hspace{2 pt}$, the greater the amount of shrinkage and thus the coefficients become more robust to collinearity. Nevertheless, the value of $\\alpha \\hspace{2 pt}$ cannot be increased indefinitely: there is an optimal point above which the variance scores and residual sum of squales drop sharply:"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "n_alphas = 200\nalphas = np.logspace(-5, 1, n_alphas)\nregr3 = linear_model.Ridge(fit_intercept=False)\n\nscores = []\nfor a in alphas:\n    regr3.set_params(alpha=a)\n    regr3.fit(X_train, y_train)\n    scores.append(regr3.score(X_test, y_test))\n\nfig = bk.figure(plot_width=640, plot_height=330,\n                title=r'Variance Scores as a function of alpha',\n                x_axis_type='log', x_range=(min(alphas), max(alphas)))\nfig.title_text_font_size = '11pt'\nfig.xaxis.axis_label = 'alpha'\nfig.yaxis.axis_label = 'scores'\nfig.axis.axis_label_text_font_size = '10pt'\nfig.line(alphas, scores, line_color='blue')\nbk.show(fig)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "### 7.2 Stochastic Gradient Descent (SGD)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "`Stochastic Gradient Descent` is a simple yet very efficient approach to discriminative learning of linear classifiers under convex loss functions such as (linear) Support Vector Machines and Logistic Regression. Even though SGD has been around in the machine learning community for a long time, it has received a considerable amount of attention just recently in the context of large-scale learning.\n\nSGD has been successfully applied to large-scale and sparse machine learning problems often encountered in text classification and natural language processing. Given that the data is sparse, the classifiers in this module easily scale to problems with more than $10^5$ training examples and more than $10^5$ features."
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "from sklearn.datasets.samples_generator import make_regression\n\nX, y = make_regression(n_samples=100000, n_features=1, n_informative=1,\\\n                        random_state=0, noise=35)\n\nreg3 = linear_model.SGDRegressor(alpha=0.1, n_iter=20)\nreg3.fit(X, y)\n\nfig = bk.figure(plot_width=630, plot_height=300, title=None)\nfig.circle(X[::1000, 0], y[::1000],  color='black')\nfig.line(X[::1000, 0], reg3.predict(X[::1000]), color='red', line_width=3)\nfig.grid.grid_line_color = None\nfig.axis.minor_tick_out = 0\nbk.show(fig)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "\n\nThis work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>."
        }
    ], 
    "nbformat_minor": 0, 
    "nbformat": 4
}