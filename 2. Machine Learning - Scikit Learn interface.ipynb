{
    "metadata": {
        "language_info": {
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }, 
            "file_extension": ".py", 
            "nbconvert_exporter": "python", 
            "mimetype": "text/x-python", 
            "pygments_lexer": "ipython3", 
            "name": "python", 
            "version": "3.5.2"
        }, 
        "kernelspec": {
            "display_name": "Python 3.5 (Experimental) with Spark 1.6", 
            "language": "python", 
            "name": "python3"
        }
    }, 
    "nbformat": 4, 
    "cells": [
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "# The scikit-learn interface\n\nAdopted from https://github.com/addfor/tutorials"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 4, 
            "source": "import scipy.io\nimport numpy as np\nimport pandas as pd"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## Defining the estimator object"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "In scikit-learn, almost all operations are done through an **estimator object**. For example, a linear regression estimator can be instantiated as follows:"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=True)\n", 
                    "name": "stdout"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 5, 
            "source": "from sklearn import linear_model\nmodel = linear_model.LinearRegression(fit_intercept=True, normalize=True)\nprint(model)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "In brackets are displayed the current values for the \u201chyperparameters\u201d of the estimator. To learn about the specific \u201chyperparameters\u201d check the documentation:"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 6, 
            "source": "# Try: model?"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Meta parameters can be changed after the model has been created:"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "LinearRegression(copy_X=True, fit_intercept=False, n_jobs=1, normalize=True)\n", 
                    "name": "stdout"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 7, 
            "source": "model.fit_intercept = False\nprint(model)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Given a scikit-learn *estimator* object named `model`, the following methods are available:\n\n* *Available in all Estimators*\n  * `model.fit()` : fit training data.\n      * For supervised learning, this accepts two arguments: `model.fit(X, y)`.\n      * For unsupervised learning, this accepts only a single argument `model.fit(X)`.\n* *Available in supervised estimators*\n  * `model.predict()` : predict the label of a new set of data. This accepts one argument `model.predict(X_new)`).\n  * `model.predict_proba()`: Returns the probability of a categorical label. The label itself is returned by `model.predict()`.\n  * `model.score()`: Scores are between 0 and 1, with a larger score indicating a better fit.\n* *Available in unsupervised estimators*\n  * `model.transform()`: Transform new data into the new basis. This accepts one argument `X_new`, and returns the new representation of the data.\n  * `model.fit_transform()`: some estimators implement this method, which more efficiently performs a fit and a transform on the same input data."
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## 1 Simple estimator example: fit a linear regression model"
        }, 
        {
            "metadata": {
                "scrolled": true, 
                "collapsed": false
            }, 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "display_data", 
                    "data": {
                        "text/html": "\n    <div class=\"bk-root\">\n        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n        <span id=\"887004de-e2c2-4223-ba73-b704044ae11a\">Loading BokehJS ...</span>\n    </div>"
                    }
                }, 
                {
                    "metadata": {}, 
                    "output_type": "display_data", 
                    "data": {
                        "application/javascript": "\n(function(global) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    window._bokeh_onload_callbacks = [];\n    window._bokeh_is_loading = undefined;\n  }\n\n\n  \n  if (typeof (window._bokeh_timeout) === \"undefined\" || force === true) {\n    window._bokeh_timeout = Date.now() + 5000;\n    window._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    if (window.Bokeh !== undefined) {\n      document.getElementById(\"887004de-e2c2-4223-ba73-b704044ae11a\").textContent = \"BokehJS successfully loaded.\";\n    } else if (Date.now() < window._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    delete window._bokeh_onload_callbacks\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    window._bokeh_onload_callbacks.push(callback);\n    if (window._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    window._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        window._bokeh_is_loading--;\n        if (window._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"887004de-e2c2-4223-ba73-b704044ae11a\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '887004de-e2c2-4223-ba73-b704044ae11a' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.4.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n      document.getElementById(\"887004de-e2c2-4223-ba73-b704044ae11a\").textContent = \"BokehJS is loading...\";\n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.4.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.4.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((window.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i](window.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < window._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!window._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      window._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"887004de-e2c2-4223-ba73-b704044ae11a\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (window._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(this));"
                    }
                }
            ], 
            "cell_type": "code", 
            "execution_count": 8, 
            "source": "import bokeh.plotting as bk\nbk.output_notebook()"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "output_type": "error", 
                    "traceback": [
                        "\u001b[1;31m\u001b[0m", 
                        "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[1;32m<ipython-input-4-028e141364df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'o'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mlin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# kernel dies due to:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;31mNameError\u001b[0m: name 'linear_model' is not defined"
                    ], 
                    "ename": "NameError", 
                    "evalue": "name 'linear_model' is not defined"
                }, 
                {
                    "metadata": {}, 
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "<matplotlib.figure.Figure at 0x7fe4a7e1ba20>", 
                        "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEACAYAAABbMHZzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVIhi+gAAAABJRU5ErkJggg==\n"
                    }
                }
            ], 
            "cell_type": "code", 
            "execution_count": 4, 
            "source": "from sklearn import datasets, preprocessing, metrics\nX, y = datasets.samples_generator.make_regression(n_samples=70,\n                                                  n_features=1, n_informative=1,\n                                                  random_state=0, noise=5)\nscaler = preprocessing.MinMaxScaler()\nX_sc = scaler.fit_transform(X)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.plot(X,y,'o')\n\nlin = linear_model.LinearRegression(fit_intercept=True)\n\n# kernel dies due to:\n#http://stackoverflow.com/questions/39720911/sgdclassifier-kernel-dies-when-using-scikit\n\n#lin.fit(X, y)\n\n#print(lin)\n#print(\"Model coefficient: %.5f, and intercept: %.5f\" % (lin.coef_, lin.intercept_))\n#err = metrics.mean_squared_error(lin.predict(X_sc), y)\n#print(\"Mean Squared Error: %.2f\" % err)"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "output_type": "error", 
                    "traceback": [
                        "\u001b[1;31m\u001b[0m", 
                        "\u001b[1;31mNotFittedError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[1;32m<ipython-input-7-6ef06e08d7ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Plot the data and the model prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0my_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m fig = bk.figure(title='Simple Regression', \n", 
                        "\u001b[1;32m/gpfs/fs01/user/s16e-7918d85e6de098-7a7840b6cba3/.local/lib/python3.5/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \"\"\"\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[0m_preprocess_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/gpfs/fs01/user/s16e-7918d85e6de098-7a7840b6cba3/.local/lib/python3.5/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36m_decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_decision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"coef_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'coo'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/gpfs/fs01/user/s16e-7918d85e6de098-7a7840b6cba3/.local/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m    688\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m         \u001b[1;31m# FIXME NotFittedError_ --> NotFittedError in 0.19\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 690\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0m_NotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    691\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;31mNotFittedError\u001b[0m: This LinearRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this method."
                    ], 
                    "ename": "NotFittedError", 
                    "evalue": "This LinearRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this method."
                }
            ], 
            "cell_type": "code", 
            "execution_count": 7, 
            "source": "\n\n# Plot the data and the model prediction\nX_p = np.linspace(0, 1, 2)[:, np.newaxis]\ny_p = lin.predict(X_p)\n\nfig = bk.figure(title='Simple Regression', \n                x_axis_label='X scaled',\n                y_axis_label='y',\n                plot_width=600, plot_height=300)\nfig.circle(X_sc.squeeze(), y, line_color='darkgreen', size=10,\n           fill_color='green', fill_alpha=0.5, legend='Measured Data')\nfig.line(X_p.ravel(), y_p, line_color='blue', legend='Predicted Values')\nfig.legend.location = 'bottom_right'\n#bk.show(fig)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## 2 Separate Training and Validation Sets"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "**Fitting a model and testing it on the same data is a methodological mistake:** a model could have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called <font color=\"Red\">overfitting</font>. To avoid it, it is common practice to hold out part of the available data as a **Validation  Set** `X_valid, y_valid`.\n\nIn scikit-learn a random split into **Training** and **Validation** sets can be quickly computed with the `train_test_split` helper function:"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "/gpfs/fs01/user/s16e-7918d85e6de098-7a7840b6cba3/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n", 
                    "name": "stderr"
                }, 
                {
                    "output_type": "error", 
                    "traceback": [
                        "\u001b[1;31m\u001b[0m", 
                        "\u001b[1;31mNotFittedError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[1;32m<ipython-input-8-378964cc77e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# kernel dies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#lin.fit(X_tr, y_tr)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mMSE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Mean Squared Error: %.2f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mMSE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model coefficient: %.5f, and intercept: %.5f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/gpfs/fs01/user/s16e-7918d85e6de098-7a7840b6cba3/.local/lib/python3.5/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \"\"\"\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[0m_preprocess_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/gpfs/fs01/user/s16e-7918d85e6de098-7a7840b6cba3/.local/lib/python3.5/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36m_decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_decision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"coef_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'coo'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/gpfs/fs01/user/s16e-7918d85e6de098-7a7840b6cba3/.local/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m    688\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m         \u001b[1;31m# FIXME NotFittedError_ --> NotFittedError in 0.19\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 690\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0m_NotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    691\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;31mNotFittedError\u001b[0m: This LinearRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this method."
                    ], 
                    "ename": "NotFittedError", 
                    "evalue": "This LinearRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this method."
                }
            ], 
            "cell_type": "code", 
            "execution_count": 8, 
            "source": "from sklearn import cross_validation\nX_tr, X_valid, y_tr, y_valid = cross_validation.train_test_split(X_sc, y,\n                                                                 test_size=0.30, random_state=0)\n# kernel dies\n#lin.fit(X_tr, y_tr)\nMSE = metrics.mean_squared_error(lin.predict(X_valid), y_valid)\nprint(\"Mean Squared Error: %.2f\" % MSE)\nprint(\"Model coefficient: %.5f, and intercept: %.5f\" % (lin.coef_, lin.intercept_))\n\nX_predicted = np.linspace(0, 1, 2)[:, np.newaxis]\ny_predicted = lin.predict(X_predicted)\nfig = bk.figure(title='Training and Validation Sets', \n                plot_width=700, plot_height=400)\nfig.circle(X_tr.squeeze(), y_tr,\n           size=5, line_alpha=0.85, fill_color='green', line_color='darkgreen',\n           legend='Training Set')\nfig.circle(X_valid.squeeze(), y_valid, \n           size=5, line_alpha=0.85, fill_color='orangered', legend='Validation Set')\nfig.line(X_predicted.ravel(), y_predicted, line_color='blue', legend='Linear Regression')\nfig.legend.location = 'bottom_right'\nbk.show(fig)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "### 2.1 Example: Do a Regression Analysis"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 91, 
            "source": "#enerate Regression Test Data 02 \nsamples = 100\nfeatures = 5\n\nX = np.random.random_sample([samples, features])\nX.shape\n\nfor i in range (samples):\n    X[i,:] = (X[i,:]*i)+i+0.1\n\nX=pd.DataFrame(X,columns=['f'+str(i+1) for i in np.arange(features)])\n\n#Calculate y as a linear combination of features with coeff. 1.5, 2.5,..., and add some noise\nnoise = 0.1;\nlin_comb = np.linspace(1.5,features+0.5,features)\ny = pd.DataFrame(np.sum((X+np.random.random_sample([samples, features])*noise)*lin_comb,axis=1),columns=['y'])\n"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 97, 
            "source": "Xt,Xv,yt,yv = cross_validation.train_test_split(X,y,test_size=0.30,random_state=0)\nXt = pd.DataFrame(Xt, columns=X.columns)\nXv = pd.DataFrame(Xv, columns=X.columns)\nyt = pd.DataFrame(yt, columns=['y'])\nyv = pd.DataFrame(yv, columns=['y'])"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 94, 
            "source": "# Scaler is fitted only on training data\nXts = Xt.copy()\nXvs = Xv.copy()\nscaler = preprocessing.StandardScaler(copy=False, with_mean=True, with_std=True).fit(Xt)\nscaler.transform(Xts)\nscaler.transform(Xv);"
        }, 
        {
            "metadata": {
                "scrolled": true, 
                "collapsed": false
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null, 
            "source": "model = linear_model.LinearRegression(fit_intercept=True, normalize=True)\nmodel.fit(Xts, yt)\n\n#dead kernel\n\n# NOTICE: coefficients are much different from 1.5, 2.5, ... when fitting on scaled data\nprint(model.coef_)\nprint(model.intercept_)"
        }, 
        {
            "metadata": {
                "scrolled": false, 
                "collapsed": false
            }, 
            "outputs": [
                {
                    "output_type": "error", 
                    "traceback": [
                        "\u001b[1;31m\u001b[0m", 
                        "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[1;32m<ipython-input-2-d36169f1b16e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Plot the data and the model prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0myp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXvs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m fig = bk.figure(title='Measured Values VS Predicted Values',\n\u001b[0;32m      4\u001b[0m                 \u001b[0mx_axis_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'y val'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_axis_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'y pred'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                 plot_width=600, plot_height=400)\n", 
                        "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
                    ], 
                    "ename": "NameError", 
                    "evalue": "name 'model' is not defined"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 2, 
            "source": "# Plot the data and the model prediction\nyp = model.predict(Xvs)\nfig = bk.figure(title='Measured Values VS Predicted Values',\n                x_axis_label='y val', y_axis_label='y pred',\n                plot_width=600, plot_height=400)\nfig.circle(yv['measured'], yp[:,0], size=8,\n           fill_color='blue', fill_alpha=0.5, line_color='black', alpha=0.2)\nbk.show(fig)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "### 2.2 Example: Training and a Validation Sets on a Classification Problem"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "The K-neighbors is an instance-based classifier that predicts the label of an unknown point based on the labels of the *K* nearest points in the parameter space:"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "[[178   0   0   0   0   0   0   0   0   0]\n [  0 182   0   0   0   0   0   0   0   0]\n [  0   0 177   0   0   0   0   0   0   0]\n [  0   0   0 183   0   0   0   0   0   0]\n [  0   0   0   0 181   0   0   0   0   0]\n [  0   0   0   0   0 182   0   0   0   0]\n [  0   0   0   0   0   0 181   0   0   0]\n [  0   0   0   0   0   0   0 179   0   0]\n [  0   0   0   0   0   0   0   0 174   0]\n [  0   0   0   0   0   0   0   0   0 180]]\n\nF1 Score:   1.0\n", 
                    "name": "stdout"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 11, 
            "source": "from sklearn import neighbors\ndigits = datasets.load_digits()\nX, y = digits.data, digits.target\n\nclf = neighbors.KNeighborsClassifier(n_neighbors=1)\nclf.fit(X, y)\ny_pred = clf.predict(X)\nprint(metrics.confusion_matrix(y_pred, y))\nprint('\\nF1 Score:  ', metrics.f1_score(y_pred, y, average='weighted'))"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Apparently, we've found a perfect classifier!  But this is misleading for the reasons we saw before: the classifier essentially \"memorizes\" all the samples it has already seen.  To really test how well this algorithm does, we need to try some samples it *hasn't* yet seen.\n\nHere we split the original data in **Training** and **Validation** sets and run again the previous algorithm. In this case we see that we still have a good classifier but the precision is reduced:"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "[[60  0  0  0  0  0  0  0  0  0]\n [ 0 73  0  0  0  0  0  0  0  0]\n [ 0  0 71  0  0  0  0  0  0  0]\n [ 0  0  0 70  0  0  0  0  0  0]\n [ 0  0  0  0 63  0  0  0  0  0]\n [ 0  0  0  0  0 87  1  0  0  1]\n [ 0  0  0  0  0  0 76  0  0  0]\n [ 0  0  0  0  0  0  0 65  0  0]\n [ 0  2  0  1  0  0  0  0 74  1]\n [ 0  0  0  2  0  1  0  0  0 71]]\n\nF1 Score:   0.987441080752\n", 
                    "name": "stdout"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 15, 
            "source": "from sklearn.model_selection import train_test_split\n\nX_tr, X_valid, y_tr, y_valid = train_test_split(X, \n                                                y,\n                                                test_size=0.40, \n                                                random_state=0)\nclf = neighbors.KNeighborsClassifier(n_neighbors=1).fit(X_tr, y_tr)\ny_pred = clf.predict(X_valid)\nprint(metrics.confusion_matrix(y_valid, y_pred))\nprint('\\nF1 Score:  ', metrics.f1_score(y_valid, y_pred, average='weighted'))"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## 3 Cross Validation (CV)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "By partitioning the available data into **Training** and **Validation** sets, we reduce the number of samples which can be used for learning the model.\n\nBy using cross-validation **(CV)** the *validation set is defined automatically by the CV algoritm. * The training set is split into k smaller sets and:\n\n* A model is trained using k-1 of the folds as training data\n* The resulting model is validated on the remaining part of the data\n\nIn this example we first calculate the Mean Squared Error on the Test DatasetSet (which is a methodological error) and then we calculate again the same MSE using a KFold CV. The MSE calculated with Cv is much larger:"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "output_type": "error", 
                    "traceback": [
                        "\u001b[1;31m\u001b[0m", 
                        "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[1;32m<ipython-input-5-870154e1b0a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m                                                   \u001b[0mn_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_informative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                                   random_state=0, noise=5)\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mlin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#kernel dies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;31mNameError\u001b[0m: name 'linear_model' is not defined"
                    ], 
                    "ename": "NameError", 
                    "evalue": "name 'linear_model' is not defined"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 5, 
            "source": "\n\nX, y = datasets.samples_generator.make_regression(n_samples=7,\n                                                  n_features=1, n_informative=1,\n                                                  random_state=0, noise=5)\nlin = linear_model.LinearRegression(fit_intercept=True)\n\n#kernel dies\n#lin.fit(X, y)\n\n\nMSE = metrics.mean_squared_error(lin.predict(X), y)\nprint('MSE Calculated on the Training Set:   ', MSE)\nMSE_CV = cross_validation.cross_val_score(lin, X, y, cv=5, scoring='mean_squared_error')\nprint(\"MSE Calculated with Cross Validation: %0.2f (+/- %0.2f)\"\n      % (MSE_CV.mean(), MSE_CV.std() * 2))"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "When the `cv` argument is an integer like `cv=10`, `cross_val_score` uses the `KFold` or `StratifiedKFold` strategies. It is also possible to use other cross validation strategies by passing a cross validation iterator instead, for instance we can use `cross_validation.ShuffleSplit` where `n_iter` is the number of re-shuffling and splitting operations:"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "output_type": "error", 
                    "traceback": [
                        "\u001b[1;31m\u001b[0m", 
                        "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[1;32m<ipython-input-11-14963c5c25cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcv_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mShuffleSplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mMSE_CV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean_squared_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m print(\"MSE Calculated with Cross Validation: %0.2f (+/- %0.2f)\"\n\u001b[0;32m      9\u001b[0m       % (MSE_CV.mean(), MSE_CV.std() * 2))\n", 
                        "\u001b[1;31mNameError\u001b[0m: name 'lin' is not defined"
                    ], 
                    "ename": "NameError", 
                    "evalue": "name 'lin' is not defined"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 11, 
            "source": "from sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import cross_val_score\n\nn_samples = X.shape[0]\ncv_iter = ShuffleSplit(n_samples,  test_size=0.2, random_state=0)\n\nMSE_CV = cross_val_score(lin, X, y, cv=cv_iter, scoring='mean_squared_error')\nprint(\"MSE Calculated with Cross Validation: %0.2f (+/- %0.2f)\"\n      % (MSE_CV.mean(), MSE_CV.std() * 2))"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "### 3.1 Cross Validation: test many estimators on the same dataset:"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "In this example *we use the \"diabetes\" dataset* to fit tree different linear models. For each model the default CV score is calculated and displayed:"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "output_type": "error", 
                    "traceback": [
                        "\u001b[1;31m\u001b[0m", 
                        "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[1;32m<ipython-input-12-f7714f5b50a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m for Model in [linear_model.LinearRegression,\n\u001b[0m\u001b[0;32m      5\u001b[0m               \u001b[0mlinear_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRidge\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m               linear_model.Lasso]:\n", 
                        "\u001b[1;31mNameError\u001b[0m: name 'linear_model' is not defined"
                    ], 
                    "ename": "NameError", 
                    "evalue": "name 'linear_model' is not defined"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 12, 
            "source": "data = datasets.load_diabetes()\nX, y = data.data, data.target\n\nfor Model in [linear_model.LinearRegression,\n              linear_model.Ridge,\n              linear_model.Lasso]:\n    model = Model()\n    print(Model.__name__, ': ', cross_validation.cross_val_score(model, X, y).mean())"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "### 3.2 Cross Validation: test many hyperparamaters and estimators on the same dataset:"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "`Lasso` and `Ridge` accept a regularization parameter `alpha`. Here we plot the CV Score for different values of alpha"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null, 
            "source": "from bokeh.models.axes import LogAxis\n\nalphas = np.logspace(-4, -1, 200)\nfig = bk.figure(title='Alpha sensitiveness on different models',\n                plot_width=800, plot_height=400,\n                x_axis_type='log', \n                x_range=(min(alphas), max(alphas)))\n\nfor Model, color in [(linear_model.Ridge, 'blue'), (linear_model.Lasso, 'green')]:\n    scores = [cross_validation.cross_val_score(Model(alpha=a), X, y, cv=5).mean()\n              for a in alphas]\n    fig.line(alphas, scores, line_color=color, legend=Model.__name__)\n\nfig.legend.location = 'bottom_left'\nbk.show(fig)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "### 3.3 Model specific Cross Validation:"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Some models can fit data for a range of parameters almost as efficiently as fitting for a single parameter. The most common parameter amenable to this strategy is the strength of the regularizer. In this case we say that we compute the regularization path of the estimator.\n\nThis is a particular case of what we are going to see in the next paragraph: **Grid Search**.\n\nModel Specific Cross Validation is supported by the following models:\n\n* **RidgeCV**\n* **RidgeClassifierCV**\n* **LarsCV**\n* **LassoLarsCV**\n* **LassoCV**\n* **ElasticNetCV**"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null, 
            "source": "for Model in [linear_model.RidgeCV, linear_model.LassoCV]:\n    model = Model(alphas=alphas, cv=5).fit(X, y)\n    print(Model.__name__, ': ', model.alpha_)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "### 3.4 Cross-validation iterators"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "* **K-fold** - divide all samples in *K* groups, if *K=n* it is equivalent to leave-one-out.\n* **Statified K-fold** - each set contains the same prercentage of the target classes as the whole set\n* **Leave-One-Out (LOO)** - all samples except one\n* **Leave-P-Out (LPO)** - create all possible training sets by removing *p* samples from the whole set\n* **Leave-One-Label-Out (LOLO)** - holds out the samples according to a third-party provided label\n* **ShuffleSplit** - shuffles than split data in train and test sets\n* **StratifiedShuffleSplit** - ShuffleSplits by preserving percentage\n* **Bootstrap** - generate independent splits and then resample with replacement each split"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null, 
            "source": "X = np.arange(20).reshape(10,2)\ny = np.arange(10)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "**KFold** split dataset into k consecutive folds (without shuffling):"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null, 
            "source": "kf = cross_validation.KFold(len(y), n_folds=5)\nfor train, test in kf:\n    print(train, test)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "**ShuffleSplit** do not guarantee that all folds will be different:"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null, 
            "source": "ss = cross_validation.ShuffleSplit(len(y), n_iter=5, test_size=0.2)\nfor train, test in ss:\n    print(train, test)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "**Bootstrap** example:"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null, 
            "source": "bs = cross_validation.LeaveOneLabelOut(range(len(y)))\nfor train, test in bs:\n    print(train, test)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## 4 Grid Search: Searching for estimator hyperparameters"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "`GridSearchCV` can optimize any parameter provided by `estimator.get_params()` by exhaustively fitting the model with any parameter combination.\n\nWhen evaluating different settings (\u201chyperparameters\u201d) for estimators.\n\n`GridSearchCV` will optimize the hyperparameters on the **TRAINING set** (whereas the **VALIDATION set** is generated by CV). After the optimization has been done, the performances of the estimator can be evaluated on a third set called **TEST set**:"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "### 4.1 Exhaustive Grid Search"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "We'll pass to `GridSearchCV` a dictionary of parameters containing a list of hyperparameters to be searched. In this case we set `verbose=2` to see an output about all the searches done by the algorithm:"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null, 
            "source": "from sklearn import svm, grid_search\n# Standardize the data\niris = datasets.load_iris()\nscaler = preprocessing.StandardScaler(copy=True, with_mean=True, with_std=True)\nX = scaler.fit_transform(iris.data)\n# Keep out the Test Set\nX_tr, X_test, y_tr, y_test = cross_validation.train_test_split(X, iris.target,\n                                                               test_size=0.30, random_state=0)\n# Define the Search Grid\nparam_grid = [{'C': [1, 10], 'kernel': ['linear']},\n              {'C': [2], 'gamma': [0.1, 0.01], 'kernel': ['rbf']}]\n# GridSearchCV\nclf = grid_search.GridSearchCV(svm.SVC(), param_grid, cv=2, n_jobs=-1, verbose=2)\nclf.fit(X_tr, y_tr)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "**TODO -** Create a Surf Plot with the accuracy values found on an hyperparameter grid using the .grid_scores_ data"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null, 
            "source": "for item in clf.grid_scores_:\n    print(item.index)\n    print(item.parameters)\n    print(item.parameters['C'])"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Then we can check the accuracy on the **Test Set**:"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null, 
            "source": "best = clf.best_estimator_\nprint('Best estimator: ', best)\nscores = cross_validation.cross_val_score(best, X_test, y_test, cv=10)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Different `score` metrics can be used while doing `GridSearchCV`:"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null, 
            "source": "X_tr, X_test, y_tr, y_test = cross_validation.train_test_split(X, \n                                                               iris.target,\n                                                               test_size=0.60, \n                                                               random_state=0)\nscores = ['precision_weighted', 'recall_weighted']\nparam_grid = [{'C': np.logspace(-3,0,4, base=2), 'kernel': ['linear']},\n              {'C': [1, 10], 'gamma': [0.1], 'kernel': ['rbf']}]\nprint('PRECISION: fraction of retrieved instances that are relevant')\nprint('RECALL:    fraction of relevant instances that are retrieved')\nfor score in scores:\n    print('\\nTuning hyper-parameters for: %s' % score)\n    clf = grid_search.GridSearchCV(svm.SVC(), param_grid, cv=5, scoring=score)\n    clf.fit(X_tr, y_tr)\n    print('\\nBest parameters set found on development (training) set:')\n    print(clf.best_estimator_)\n    print('\\nGrid Scores on development set:')\n    for item in clf.grid_scores_:\n        print(item)\n    print('\\nDetailed classification report (on Testing Set)')\n    y_true, y_pred = y_test, clf.predict(X_test)\n    print(metrics.classification_report(y_true, y_pred))\n    print('-----------------------------------------------------')\n        "
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "### 4.2 Randomized Parameter Optimization"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "`RandomizedSearchCV` implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits:\n\n* A budget can be chosen independent of the number of parameters and possible values.\n* Adding parameters that do not influence the performance does not decrease efficiency.\n\nSpecifying how parameters should be sampled is done using a dictionary. The number of sampling iterations, is specified using the `n_iter` parameter. For each parameter, either a distribution over possible values or a list of discrete choices (which will be sampled uniformly) can be specified. This is a little example of the many statistical distributions available in `scipy`:"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null, 
            "source": "import scipy.stats\nfrom bokeh.charts import Histogram\n\nw, h = 350, 300\n\ndists = [\n    scipy.stats.expon(scale=100), # the frozed uniform distribution\n    scipy.stats.norm(loc=40, scale=10), # the frozed uniform distribution\n    scipy.stats.uniform(loc=20, scale=100), # the frozed uniform distribution\n    scipy.stats.beta(2,2,loc=20, scale=100) # the frozed uniform distribution\n]\n\nfigs = [ Histogram(dist.rvs(1e4), bins=50, \n                   width=w, height=h, \n                   xlabel=None, ylabel=None)\n        for dist in dists ]\n\nbk.show(bk.gridplot([[figs[0], figs[1]],\n                     [figs[2], figs[3]] \n                     ]))"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>."
        }
    ], 
    "nbformat_minor": 0
}