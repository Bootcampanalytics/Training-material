{
    "nbformat_minor": 0, 
    "metadata": {
        "language_info": {
            "pygments_lexer": "ipython2", 
            "name": "python", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }, 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "2.7.11"
        }, 
        "kernelspec": {
            "language": "python", 
            "name": "python2", 
            "display_name": "Python 2 with Spark 1.6"
        }
    }, 
    "nbformat": 4, 
    "cells": [
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "from six.moves import urllib\nimport tarfile\n#os.makedirs(\"enron\")    \n#url = 'https://www.cs.cmu.edu/~./enron/enron_mail_20150507.tgz'\n\n#filepath, _ = urllib.request.urlretrieve(url, 'enron_mail_20150507.tgz')\nwith tarfile.open(\"enron_mail_20150507.tgz\") as tar:\n    tar.extractall(path=\"enron\")\n", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 19, 
            "metadata": {
                "collapsed": false, 
                "scrolled": true
            }, 
            "source": "%%bash\ncd enron\ndir", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 119, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "# https://github.com/rkadlec/ubuntu-ranking-dataset-creator\n\nimport argparse\nimport os\nimport unicodecsv\nimport random\nfrom six.moves import urllib\nimport tarfile\nimport csv\n\nimport nltk\nfrom nltk.stem import SnowballStemmer, WordNetLemmatizer\n\n__author__ = 'rkadlec'\n\n\"\"\"\nScript for generation of train, test and valid datasets from Ubuntu Corpus 1 on 1 dialogs.\nCopyright IBM Corporation 2016\nLICENSE: Apache License 2.0  URL: ttp://www.apache.org/licenses/LICENSE-2.0\nContact: Rudolf Kadlec (rudolf_kadlec@cz.ibm.com)\n\"\"\"\n\ndialog_end_symbol = \"__dialog_end__\"\nend_of_utterance_symbol = \"__eou__\"\nend_of_turn_symbol = \"__eot__\"\n\n\n\ndef translate_dialog_to_lists(dialog_filename):\n    \"\"\"\n    Translates the dialog to a list of lists of utterances. In the first\n    list each item holds subsequent utterances from the same user. The second level\n    list holds the individual utterances.\n    :param dialog_filename:\n    :return:\n    \"\"\"\n\n    dialog_file = open(dialog_filename, 'r')\n    dialog_reader = unicodecsv.reader(dialog_file, delimiter='\\t',quoting=csv.QUOTE_NONE)\n\n    # go through the dialog\n    first_turn = True\n    dialog = []\n    same_user_utterances = []\n    #last_user = None\n    dialog.append(same_user_utterances)\n\n    for dialog_line in dialog_reader:\n\n        if first_turn:\n            last_user = dialog_line[1]\n            first_turn = False\n\n        if last_user != dialog_line[1]:\n            # user has changed\n            same_user_utterances = []\n            dialog.append(same_user_utterances)\n\n        same_user_utterances.append(dialog_line[3])\n\n        last_user = dialog_line[1]\n\n    dialog.append([dialog_end_symbol])\n\n    return dialog\n\n\ndef get_random_utterances_from_corpus(candidate_dialog_paths,rng,utterances_num=9,min_turn=3,max_turn=20):\n    \"\"\"\n    Sample multiple random utterances from the whole corpus.\n    :param candidate_dialog_paths:\n    :param rng:\n    :param utterances_num: number of utterances to generate\n    :param min_turn: minimal index of turn that the utterance is selected from\n    :return:\n    \"\"\"\n    utterances = []\n    dialogs_num = len(candidate_dialog_paths)\n\n    for i in xrange(0,utterances_num):\n        # sample random dialog\n        dialog_path = candidate_dialog_paths[rng.randint(0,dialogs_num-1)]\n        # load the dialog\n        dialog = translate_dialog_to_lists(dialog_path)\n\n        # we do not count the last  _dialog_end__ urn\n        dialog_len = len(dialog) - 1\n        if(dialog_len<min_turn):\n            print \"Dialog {} was shorter than the minimum required lenght {}\".format(dialog_path,dialog_len)\n            exit()\n        # sample utterance, exclude the last round that is always \"dialog end\"\n        max_ix = min(max_turn, dialog_len) - 1\n\n        # this code deals with corner cases when dialogs are too short\n        if min_turn -1 == max_ix:\n            turn_index = max_ix\n        else:\n            turn_index = rng.randint(min_turn,max_ix)\n\n        utterance = singe_user_utterances_to_string(dialog[turn_index])\n        utterances.append(utterance)\n    return utterances\n\ndef singe_user_utterances_to_string(utterances_list):\n    \"\"\"\n    Concatenates multiple user's utterances into a single string.\n    :param utterances_list:\n    :return:\n    \"\"\"\n    return \" \".join(map(lambda x: x+\" \"+ end_of_utterance_symbol, utterances_list))\n\ndef dialog_turns_to_string(dialog):\n    \"\"\"\n    Translates the whole dialog (list of lists) to a string\n    :param dialog:\n    :return:\n    \"\"\"\n    # join utterances\n    turns_as_strings = map(singe_user_utterances_to_string,dialog)\n    # join turns\n    return \"\".join(map(lambda x : x + \" \" + end_of_turn_symbol + \" \", turns_as_strings))\n\ndef create_random_context(dialog,rng,minimum_context_length=2,max_context_length=20):\n    \"\"\"\n    Samples random context from a dialog. Contexts are uniformly sampled from the whole dialog.\n    :param dialog:\n    :param rng:\n    :return: context, index of next utterance that follows the context\n    \"\"\"\n    # sample dialog context\n    #context_turns = rng.randint(minimum_context_length,len(dialog)-1)\n    max_len = min(max_context_length, len(dialog)) - 2\n    if max_len <= minimum_context_length:\n        context_turns = max_len\n    else:\n        context_turns = rng.randint(minimum_context_length,max_len)\n\n    # create string\n    return dialog_turns_to_string(dialog[:context_turns]),context_turns\n\n\n\ndef create_single_dialog_train_example(context_dialog_path, candidate_dialog_paths, rng, positive_probability,\n                                       minimum_context_length=2,max_context_length=20):\n    \"\"\"\n    Creates a single example for training set.\n    :param context_dialog_path:\n    :param candidate_dialog_paths:\n    :param rng:\n    :param positive_probability:\n    :return:\n    \"\"\"\n\n    dialog = translate_dialog_to_lists(context_dialog_path)\n\n    context_str, next_utterance_ix = create_random_context(dialog, rng,\n                                                           minimum_context_length=minimum_context_length,\n                                                           max_context_length=max_context_length)\n\n    if positive_probability > rng.random():\n        # use the next utterance as positive example\n        response = singe_user_utterances_to_string(dialog[next_utterance_ix])\n        label = 1.0\n    else:\n        response = get_random_utterances_from_corpus(candidate_dialog_paths,rng,1,\n                                                     min_turn=minimum_context_length+1,\n                                                     max_turn=max_context_length)[0]\n        label = 0.0\n    return context_str, response, label\n\n\ndef create_single_dialog_test_example(context_dialog_path, candidate_dialog_paths, rng, distractors_num, max_context_length):\n    \"\"\"\n    Creates a single example for testing or validation. Each line contains a context, one positive example and N negative examples.\n    :param context_dialog_path:\n    :param candidate_dialog_paths:\n    :param rng:\n    :param distractors_num:\n    :return: triple (context, positive response, [negative responses])\n    \"\"\"\n\n    dialog = translate_dialog_to_lists(context_dialog_path)\n\n    context_str, next_utterance_ix = create_random_context(dialog, rng, max_context_length=max_context_length)\n\n    # use the next utterance as positive example\n    positive_response = singe_user_utterances_to_string(dialog[next_utterance_ix])\n\n    negative_responses = get_random_utterances_from_corpus(candidate_dialog_paths,rng,distractors_num)\n    return context_str, positive_response, negative_responses\n\n\ndef create_examples_train(candidate_dialog_paths, rng, positive_probability=0.5, max_context_length=20):\n    \"\"\"\n    Creates single training example.\n    :param candidate_dialog_paths:\n    :param rng:\n    :param positive_probability: probability of selecting positive training example\n    :return:\n    \"\"\"\n    i = 0\n    examples = []\n    for context_dialog in candidate_dialog_paths:\n        if i % 1000 == 0:\n            print str(i)\n        dialog_path = candidate_dialog_paths[i]\n        examples.append(create_single_dialog_train_example(dialog_path, candidate_dialog_paths, rng, positive_probability,\n                                                           max_context_length=max_context_length))\n        i+=1\n    #return map(lambda dialog_path : create_single_dialog_train_example(dialog_path, candidate_dialog_paths, rng, positive_probability), candidate_dialog_paths)\n\ndef create_examples(candidate_dialog_paths, examples_num, creator_function):\n    \"\"\"\n    Creates a list of training examples from a list of dialogs and function that transforms a dialog to an example.\n    :param candidate_dialog_paths:\n    :param creator_function:\n    :return:\n    \"\"\"\n    i = 0\n    examples = []\n    unique_dialogs_num = len(candidate_dialog_paths)\n\n    while i < examples_num:\n        context_dialog = candidate_dialog_paths[i % unique_dialogs_num]\n        # counter for tracking progress\n        if i % 1000 == 0:\n            print str(i)\n        i+=1\n\n        examples.append(creator_function(context_dialog, candidate_dialog_paths))\n\n    return examples\n\ndef convert_csv_with_dialog_paths(csv_file):\n    \"\"\"\n    Converts CSV file with comma separated paths to filesystem paths.\n    :param csv_file:\n    :return:\n    \"\"\"\n    def convert_line_to_path(line):\n        file, dir = map(lambda x : x.strip(), line.split(\",\"))\n        return os.path.join(dir, file)\n\n    return map(convert_line_to_path, csv_file)\n\n\ndef prepare_data_maybe_download(directory):\n  \"\"\"\n  Download and unpack dialogs if necessary.\n  \"\"\"\n  filename = 'ubuntu_dialogs.tgz'\n  url = 'http://cs.mcgill.ca/~jpineau/datasets/ubuntu-corpus-1.0/ubuntu_dialogs.tgz'\n  dialogs_path = os.path.join(directory, 'dialogs')\n\n  # test it there are some dialogs in the path\n  if not os.path.exists(os.path.join(directory,\"10\",\"1.tst\")):\n    # dialogs are missing\n    archive_path = os.path.join(directory,filename)\n    if not os.path.exists(archive_path):\n        # archive missing, download it\n        print(\"Downloading %s to %s\" % (url, archive_path))\n        filepath, _ = urllib.request.urlretrieve(url, archive_path)\n        print \"Successfully downloaded \" + filepath\n\n    # unpack data\n    if not os.path.exists(dialogs_path):\n          print(\"Unpacking dialogs ...\")\n          with tarfile.open(archive_path) as tar:\n                tar.extractall(path=directory)\n          print(\"Archive unpacked.\")\n\n    return\n\n\n\ndef prepare_file_lists(file_list):\n    \"\"\"\n    Download filelists.\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/rkadlec/ubuntu-ranking-dataset-creator/master/src/meta/' + file_list\n    path=%pwd\n    filepath = os.path.join(path, 'dialogs/',file_list)\n\n    print(\"Downloading %s to %s\" % (url, filepath))\n    filepath, _ = urllib.request.urlretrieve(url, filepath)\n    print \"Successfully downloaded \" + filepath\n\n    return\n\n\n\n#prepare_data_maybe_download(%pwd)\n#prepare_file_lists('trainfiles.csv')\n#prepare_file_lists('testfiles.csv')\n#prepare_file_lists('valfiles.csv')\n", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "def translate_dialog_to_lists(dialog_filename):\n    \"\"\"\n    Translates the dialog to a list of lists of utterances. In the first\n    list each item holds subsequent utterances from the same user. The second level\n    list holds the individual utterances.\n    :param dialog_filename:\n    :return:\n    \"\"\"\n\ndef get_random_utterances_from_corpus(candidate_dialog_paths,rng,utterances_num=9,min_turn=3,max_turn=20):\n    \"\"\"\n    Sample multiple random utterances from the whole corpus.\n    :param candidate_dialog_paths:\n    :param rng:\n    :param utterances_num: number of utterances to generate\n    :param min_turn: minimal index of turn that the utterance is selected from\n    :return:\n    \"\"\"\n\ndef singe_user_utterances_to_string(utterances_list):\n    \"\"\"\n    Concatenates multiple user's utterances into a single string.\n    :param utterances_list:\n    :return:\n    \"\"\"\n\ndef dialog_turns_to_string(dialog):\n    \"\"\"\n    Translates the whole dialog (list of lists) to a string\n    :param dialog:\n    :return:\n    \"\"\"\n\ndef create_random_context(dialog,rng,minimum_context_length=2,max_context_length=20):\n    \"\"\"\n    Samples random context from a dialog. Contexts are uniformly sampled from the whole dialog.\n    :param dialog:\n    :param rng:\n    :return: context, index of next utterance that follows the context\n    \"\"\"\n\n\ndef create_single_dialog_train_example(context_dialog_path, candidate_dialog_paths, rng, positive_probability,\n                                       minimum_context_length=2,max_context_length=20):\n    \"\"\"\n    Creates a single example for training set.\n    :param context_dialog_path:\n    :param candidate_dialog_paths:\n    :param rng:\n    :param positive_probability:\n    :return:\n    \"\"\"\n\ndef create_single_dialog_test_example(context_dialog_path, candidate_dialog_paths, rng, distractors_num, max_context_length):\n    \"\"\"\n    Creates a single example for testing or validation. Each line contains a context, one positive example and N negative examples.\n    :param context_dialog_path:\n    :param candidate_dialog_paths:\n    :param rng:\n    :param distractors_num:\n    :return: triple (context, positive response, [negative responses])\n    \"\"\"\n\ndef create_examples_train(candidate_dialog_paths, rng, positive_probability=0.5, max_context_length=20):\n    \"\"\"\n    Creates single training example.\n    :param candidate_dialog_paths:\n    :param rng:\n    :param positive_probability: probability of selecting positive training example\n    :return:\n    \"\"\"\n\ndef create_examples(candidate_dialog_paths, examples_num, creator_function):\n    \"\"\"\n    Creates a list of training examples from a list of dialogs and function that transforms a dialog to an example.\n    :param candidate_dialog_paths:\n    :param creator_function:\n    :return:\n    \"\"\"\n\ndef convert_csv_with_dialog_paths(csv_file):\n    \"\"\"\n    Converts CSV file with comma separated paths to filesystem paths.\n    :param csv_file:\n    :return:\n    \"\"\"\n\ndef prepare_data_maybe_download(directory):\n  \"\"\"\n  Download and unpack dialogs if necessary.\n  \"\"\"\n\ndef prepare_file_lists(file_list):\n    \"\"\"\n    Download filelists.\n    \"\"\"\n", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 125, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "i=10000", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 299, 
            "metadata": {
                "collapsed": false, 
                "scrolled": true
            }, 
            "source": "i=i+1\ntranslate_dialog_to_lists(\"10/\"+str(i)+\".tsv\")\n\n", 
            "outputs": [
                {
                    "execution_count": 299, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[[u'oops'],\n [u\"you won't be able to run games like that in Wine unless the game is very portable\"],\n [u'lols thanks for noticing, ive installed a prefix for it and moved the folder',\n  u'only thing im missing now is graphic drivers, which i dont think are supported with my hardware'],\n [u'there is a PPA with a legacy driver, may help',\n  u'http://www.ashwinraon.com/2012/10/amd-legacy-driver-solution-for-ubuntu-12-10/'],\n [u'the problem i seem to be having is that HP has modified this chipset',\n  u'on windows i had to go to hp to get the driver for xp and install in compatibility mode'],\n [u'oh jeez, sounds brilliant'],\n [u'yeah considering ati has drivers that support this card in linux, while hp does not'],\n ['__dialog_end__']]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 42, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "path=%pwd\nfile_list = path + '/dialogs/'+ 'trainfiles.csv'\nf = open(os.path.join(file_list), 'r')\ndialogs_paths =convert_csv_with_dialog_paths(f)\n", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 114, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "import random\nrng = random.Random()\n#os.chdir('dialogs/')\nget_random_utterances_from_corpus(dialogs_paths,rng,utterances_num=20,min_turn=3,max_turn=20)\n", 
            "outputs": [
                {
                    "execution_count": 114, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[u'which is why you have 6 months.  no other linux distribution does shorter, either. __eou__',\n u\"you're right, its not there. Can you tell me please how i add it? Im noob. thanks __eou__\",\n u'on ubuntu i use quite light theme. the problem is that it uses lots and lots of ram and processor. if i turn on FF and NetBeans, my PC is finished __eou__',\n u'ok let me check it out __eou__',\n u'thnks __eou__',\n u'no you need to keep windows too :) __eou__  well get your cd/rw working man :) __eou__',\n u\"yeah, it doesn't delete, but after awhile they will be deleted - I think they might get deleted on boot __eou__\",\n u\"we can't help you if we don't know the error that you are encountering __eou__\",\n u'that could be because the problem depends on which hardware is in use. __eou__',\n u'na, 6600 is too... the same as what i have now __eou__',\n u' thanks :) __eou__',\n u'does it run basically the same? i need to be able to use rythmbox to sync an ipod, and be able to play mp3s as well as run an occasional windows program with wine __eou__',\n u\"Ok, I'll install the gtk headers then __eou__\",\n u\"That sounds like a great idea.  I'll google it. __eou__\",\n u'fantastic :) __eou__',\n u\"powertool, I don't have much experience with firewalls on linux. Can I shut it off if I am not using the vbox? My previous experience seemed to be that I couldn't. __eou__ I thought of that but if my XP gets a virus, it can subvert the firewall. __eou__\",\n u'no __eou__ https://wiki.ubuntu.com/Releases __eou__',\n u'yup __eou__',\n u'how do I find out which device it is? __eou__',\n u'nvu __eou__']"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "#####################################################################################\n# Command line script related code\n#####################################################################################\n\nif __name__ == '__main__':\n\n\n    def create_eval_dataset(args, file_list_csv):\n        rng = random.Random(args.seed)\n        # training dataset\n        f = open(os.path.join(\"meta\", file_list_csv), 'r')\n        dialog_paths = map(lambda path: os.path.join(args.data_root, \"dialogs\", path), convert_csv_with_dialog_paths(f))\n\n        data_set = create_examples(dialog_paths,\n                                   len(dialog_paths),\n                                   lambda context_dialog, candidates : create_single_dialog_test_example(context_dialog, candidates, rng,\n                                                                     args.n, args.max_context_length))\n        # output the dataset\n        w = unicodecsv.writer(open(args.output, 'w'), encoding='utf-8')\n        # header\n        header = [\"Context\", \"Ground Truth Utterance\"]\n        header.extend(map(lambda x: \"Distractor_{}\".format(x), xrange(args.n)))\n        w.writerow(header)\n\n        stemmer = SnowballStemmer(\"english\")\n        lemmatizer = WordNetLemmatizer()\n\n        for row in data_set:\n            translated_row = [row[0], row[1]]\n            translated_row.extend(row[2])\n            \n            if args.tokenize:\n                translated_row = map(nltk.word_tokenize, translated_row)\n                if args.stem:\n                    translated_row = map(lambda sub: map(stemmer.stem, sub), translated_row)\n                if args.lemmatize:\n                    translated_row = map(lambda sub: map(lambda tok: lemmatizer.lemmatize(tok, pos='v'), sub), translated_row)\n                    \n                translated_row = map(lambda x: \" \".join(x), translated_row)\n\n            w.writerow(translated_row)\n        print(\"Dataset stored in: {}\".format(args.output))\n\n\n    def train_cmd(args):\n\n        rng = random.Random(args.seed)\n        # training dataset\n\n        f = open(os.path.join(\"meta\", \"trainfiles.csv\"), 'r')\n        dialog_paths = map(lambda path: os.path.join(args.data_root, \"dialogs\", path), convert_csv_with_dialog_paths(f))\n\n        train_set = create_examples(dialog_paths,\n                                    args.examples,\n                                    lambda context_dialog, candidates :\n                                    create_single_dialog_train_example(context_dialog, candidates, rng,\n                                                                       args.p, max_context_length=args.max_context_length))\n\n        stemmer = SnowballStemmer(\"english\")\n        lemmatizer = WordNetLemmatizer()\n\n        # output the dataset\n        w = unicodecsv.writer(open(args.output, 'w'), encoding='utf-8')\n        # header\n        w.writerow([\"Context\", \"Utterance\", \"Label\"])\n        for row in train_set:\n            translated_row = row\n\n            if args.tokenize:\n                translated_row = [nltk.word_tokenize(row[i]) for i in [0,1]]\n\n                if args.stem:\n                    translated_row = map(lambda sub: map(stemmer.stem, sub), translated_row)\n                if args.lemmatize:\n                    translated_row = map(lambda sub: map(lambda tok: lemmatizer.lemmatize(tok, pos='v'), sub), translated_row)\n\n                translated_row = map(lambda x: \" \".join(x), translated_row)\n                translated_row.append(int(float(row[2])))\n\n            w.writerow(translated_row)\n        print(\"Train dataset stored in: {}\".format(args.output))\n\n    def valid_cmd(args):\n        create_eval_dataset(args, \"valfiles.csv\")\n\n    def test_cmd(args):\n        create_eval_dataset(args, \"testfiles.csv\")\n\n\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n                                     description=\"Script that creates train, valid and test set from 1 on 1 dialogs in Ubuntu Corpus. \" +\n                                                 \"The script downloads 1on1 dialogs from internet and then it randomly samples all the datasets with positive and negative examples.\")\n\n    parser.add_argument('--data_root', default='.',\n                        help='directory where 1on1 dialogs will be downloaded and extracted, the data will be downloaded from cs.mcgill.ca/~jpineau/datasets/ubuntu-corpus-1.0/ubuntu_dialogs.tgz')\n\n    parser.add_argument('--seed', type=int, default=1234,\n                        help='seed for random number generator')\n\n    parser.add_argument('--max_context_length', type=int, default=20,\n                        help='maximum number of dialog turns in the context')\n\n    parser.add_argument('-o', '--output', default=None,\n                        help='output csv')\n\n    parser.add_argument('-t', '--tokenize', action='store_true',\n                        help='tokenize the output')\n\n    parser.add_argument('-l', '--lemmatize', action='store_true',\n                        help='lemmatize the output by nltk.stem.WorldNetLemmatizer (applied only when -t flag is present)')\n\n    parser.add_argument('-s', '--stem', action='store_true',\n                        help='stem the output by nltk.stem.SnowballStemmer (applied only when -t flag is present)')\n\n    subparsers = parser.add_subparsers(help='sub-command help')\n\n    parser_train = subparsers.add_parser('train', help='trainset generator')\n    parser_train.add_argument('-p', type=float, default=0.5, help='positive example probability')\n    parser_train.add_argument('-e', '--examples', type=int, default=1000000, help='number of examples to generate')\n    parser_train.set_defaults(func=train_cmd)\n\n    parser_test = subparsers.add_parser('test', help='testset generator')\n    parser_test.add_argument('-n', type=int, default=9, help='number of distractor examples for each context')\n    parser_test.set_defaults(func=test_cmd)\n\n    parser_valid = subparsers.add_parser('valid', help='validset generator')\n    parser_valid.add_argument('-n', type=int, default=9, help='number of distractor examples for each context')\n    parser_valid.set_defaults(func=valid_cmd)\n\n    args = parser.parse_args()\n\n    # download and unpack data if necessary\n    prepare_data_maybe_download(args.data_root)\n\n    # create dataset\n    args.func(args)\n", 
            "outputs": [], 
            "cell_type": "code"
        }
    ]
}